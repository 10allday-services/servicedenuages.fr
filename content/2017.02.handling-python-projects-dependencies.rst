Handling Python Project dependencies
####################################

:slug: handling-python-projects-dependencies
:date: 2017-02-24
:lang: en
:url: handling-python-projects-dependencies
:summary:
    Coping with project depdendencies can be a bit cumbersome.
	Learn how we are handling our Python project dependencies.


Introduction
============

If you ever started a Python project you might have heard of a
``requirements.txt`` file that contains a list of dependencies.

.. code-block:: ini

    pelican
    beautifulsoup4
    ghp-import
    Pillow==4.0.0
    markdown
    pysvg>=0.2,<0.3


However if you plan on ship your project or library with Pypi, install
it in another project or in a virtualevn you need to put them in the
``setup.py`` file in order for them to be installed automatically with
your code.

.. code-block:: ini

    setup(name='saucisson',
      version='1.0.0.dev0',
      license='Apache License (2.0)',
      packages=find_packages(),
      include_package_data=True,
      zip_safe=False,
      install_requires=[
        'colander >= 1.3.2',
        'colorama',
        'cornice >= 2.4',
        'cornice_swagger >= 0.5',
        'jsonschema',
        'jsonpatch',
        'python-dateutil',
        'pyramid > 1.8',
        'pyramid_multiauth >= 0.8',  # User on policy selected event.
        'transaction',
        'pyramid_tm',
        'requests',
        'structlog >= 16.1.0',
        'waitress',
        'ujson >= 1.35'])

This raises a lot of questions:

 - How do I update project dependencies?
 - What were the known working dependencies version at a certain release?
 - How do I make sure a release of one of my dependencies doesn't break my release?
 - How do I make sure that my code is not used with a unsupported version of a dependency?


Avoid using requirements file to handle your program dependencies
=================================================================

Once your program (i.e ``saucisson``) is released, users will install
it with ``pip install saucisson``, which means that
``python setup.py install`` needs to pick up the right dependencies for it.

It means that ``setup.py`` needs to know what are your program
dependencies and not the ``requirements.txt`` file. Instead use your
``requirements.txt`` to set a know working set of your dependencies at
the time of a release.

We usually add a `build-requirements` makefile rule on our projects:

.. code-block:: Makefile

    VIRTUALENV = virtualenv --python=python3
    
    
    build-requirements:
    	$(eval TEMPDIR := $(shell mktemp -d))
    	$(VIRTUALENV) $(TEMPDIR)
    	$(TEMPDIR)/bin/pip install -U pip  # Upgrade pip
    	$(TEMPDIR)/bin/pip install -Ue .   # Develop the current project locally
    	# Freeze the dependencies ignoring the dependency links.
    	$(TEMPDIR)/bin/pip freeze | grep -v -- '^-e' > requirements.txt

If you run `make build-requirements` before tagging your release you
will document what was the known working dependency set at the time of
the release.

Then you can install your program later using this file as a
`dependency constrain file <https://pip.pypa.io/en/stable/user_guide/#constraints-files>`_:

.. code-block:: console

    pip install saucisson -c requirements.txt


Avoid pinning version in your setup.py
======================================

You might be tempted to put dependencies versions in your setup.py,
i.e. "pinning" them to a specific version.

You don't need to do so because you are using a contraint file, you
are safe for future updates that might break your code.

On your CI, don't use a contraint file. It will help you to detect
that you need to take some actions to support the new released version
that break your tests.

However with that in mind, there are some cases when you still want to
pin some version:


If you know that your project will not work with a lower version
----------------------------------------------------------------

If you are using a feature or API that didn't exist before or you hit
a bug that was fixed later.

.. code-block::

   psycopg > 2.5
   colander >= 1.3.2
   ujson >= 1.35

It won't change the way pip handles your dependency, because even if
you don't put this, pip will always try to install the latest version.

However it will detect if another library or the project using your
library is trying to use it with a lower version that won't work with
your code.


If you know that your project doesn't support yet the next release
------------------------------------------------------------------

**I insist that this applies only if a new version version of a
dependency has already been released and that your test suite doesn't
run on it.**

In that case, and only in that case, you can pin the dependency's
version for the shortest possible time until you port your project to
it.

It's common to encounter breaking changes when upgrading frameworks:

.. code-block::

   Pyramid < 1.8
   django >1.6,<= 1.8

The danger of doing it is that you might create
``pkg_resources.VersionConflict`` errors.

When Python starts and imports your lib it will look at the
requirements list and validate that all dependencies are installed
with their expected version. If it is not the case Python will not let
you start your application.

However when you install a dependency, pip will check if it is already
installed without validating if the expected version is installed but
rather if a version is installed.

If a lib already installed the dependency with a greater version in
your virtualenv, pip will not upgrade it with the mandatory lower
version.

An easy way to break things is to pin a max ``requests`` version for
instance:

.. code-block::

   requests < 2.13

If you do that, you will end up having
``pkg_resources.VersionConflict`` error when running your program.

What is happening is that Python is checking the dependencies and will
refuse to run if you have a greater ``requests`` version.

This can happen if another dependency already needed ``requests`` as a
dependency and pip already installed it with a greater version.

**So really do it only if you must.**


What about test dependencies?
=============================

That's a good question, I am glad you asked.

Test dependencies are less of an issue. You can either use
``test_requires`` in your ``setup.py`` or a ``dev-requirements.txt``
file.

In the former you will need to run tests using ``python setup.py test``
which unfortunately doesn't run in a ``virtualenv``

In the later you will need to make sure your test dependencies are
installed before running the tests but tools like ``tox`` already do
that for you.


Our take on this is to put test dependencies in a
``dev-requirements.txt`` file.

We have a Makefile rule that knows if dev-dependencies should be
installed or not before running the ``tests`` target.

As a bonus it will automatically create a virtualenv if you don't
have one already activated:

.. code-block:: Makefile

    VIRTUALENV = virtualenv --python=python3
    VENV := $(shell echo $${VIRTUAL_ENV-.venv})  # Use the activated virtualenv path or use .venv
    PYTHON = $(VENV)/bin/python
    INSTALL_STAMP = $(VENV)/.install.stamp
    INSTALL_DEV_STAMP = $(VENV)/.dev_env_installed.stamp
    
    install: $(INSTALL_STAMP)
    
    $(INSTALL_STAMP): $(PYTHON) setup.py  # Refresh the virtualenv if setup.py changed
    	$(VENV)/bin/pip install -U pip
    	$(VENV)/bin/pip install -Ue .
    	touch $(INSTALL_STAMP)
    
    virtualenv: $(PYTHON)  # Create the virtualenv if needed (python executable not present)
    
    $(PYTHON):
    	$(VIRTUALENV) $(VENV)
    	
    install-dev: install $(DEV_STAMP)
    
    # Refresh dev dependencies if dev-requirements.txt changed
    $(DEV_STAMP): $(PYTHON) dev-requirements.txt
    	$(VENV)/bin/pip install -Ur dev-requirements.txt
    	touch $(DEV_STAMP)
    
    tests-once: install-dev
        $(VENV)/bin/py.test tests/
    
    serve:
    	$(VENV)/bin/python manage.py runserver
